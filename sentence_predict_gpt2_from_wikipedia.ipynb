{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentence predict gpt2 from wikipedia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randywreed/gpt2_lda/blob/master/sentence_predict_gpt2_from_wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P98_trwB_uf9"
      },
      "source": [
        "## Setup (always run this)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0olZRWVRukCk",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IwwhHOCFAFX5"
      },
      "source": [
        "## MODIFY THE FOLLOWING CELLS: \n",
        "You must have already created a folder in your google drive for use<br>\n",
        "The next two cells give colab permission to use your google drive (you will do this twice)<br>\n",
        "In the following cell change the *paulaidir* to google drive directory to store models and data<br>\n",
        "change *trans* to the tranlation (should correspond to the .csv file name)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IrmzDJuqvAeh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1bf0b0eb-8033-4adc-b757-898678b6b106"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQoukQFF4KEY",
        "colab": {}
      },
      "source": [
        "#set paulaidir to you google drive directory\n",
        "#trans should be equivalent to the .csv file for the full translation (i.e. niv.csv)\n",
        "#if you need to see a list of csv files continue down, and then come back, change and re-run this cell\n",
        "#this needs to be connect to the Religion of GPT2/fastai folder (Click the folder icon on the right and surf to Religion of GPT2 and the fastai \n",
        "#- click the hamburger and \"copy path\" add an ending /)\n",
        "paulaidir='/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/' #gdrive\n",
        "aidatadir=paulaidir #gdrive\n",
        "#paulaidir='/spell/fastai/' #spell\n",
        "#aidatadir='/spell/gpt2wikidata/' #spell\n",
        "trans=\"wikipedia_relwcat - wikipedia_relwcat\"\n",
        "myemail=\"waasak@appstate.edu\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "12OUvVdsukDg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e69a3bb7-6b64-4423-a9da-aa353ae2d09f"
      },
      "source": [
        "from pathlib import Path\n",
        "paulpath=Path(paulaidir)\n",
        "print(paulpath)\n",
        "#path=Path(paulaidir+'fastai/')\n",
        "path=Path(paulaidir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vbD4pG7J1heu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f00de413-5120-40c2-e858-a190354fcde5"
      },
      "source": [
        "#to see a list of .csv files remove the # from the following lines of the cell\n",
        "import glob\n",
        "for filename in glob.glob(str(aidatadir)+\"*\"):\n",
        "  print(filename)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/data\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/models\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/myconfig.yml\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/lm_databunch_wikipedia_relwcat - wikipedia_relwcat\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/model\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/bible_textlist_class_wikipedia_relwcat - wikipedia_relwcat\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/fastai\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/wikirel_sec.pkl\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/wiki_textlist_class_wikipedia_relwcat - wikipedia_relwcat\n",
            "/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/clas_wiki_textlist_class_wikipedia_relwcat - wikipedia_relwcat.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Yrb_UgDukDj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "129812d8-aa2c-4cd8-9984-064e385bc66f"
      },
      "source": [
        "translation=trans+\".csv\"  \n",
        "file=aidatadir+translation\n",
        "file"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/wikipedia_relwcat - wikipedia_relwcat.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tWOHr8LSukDC",
        "colab": {}
      },
      "source": [
        "from fastai.distributed import *\n",
        "import argparse"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dY5-_USvukDF",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3IzlDAAukDH",
        "colab": {}
      },
      "source": [
        "from fastai.callbacks import SaveModelCallback"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ghKapI0ukDP",
        "colab": {}
      },
      "source": [
        "# import fastai.utils.collect_env\n",
        "\n",
        "# fastai.utils.collect_env.show_install()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hVG8jD8wukDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9c2e2235-6800-4963-f72e-88bbd679f24f"
      },
      "source": [
        "Config.DEFAULT_CONFIG={\n",
        "    'data_path': aidatadir+'data',\n",
        "    'model_path': paulaidir+'model'\n",
        "}\n",
        "print(Config.DEFAULT_CONFIG)\n",
        "Config.create(paulaidir+'fastai/myconfig.yml')\n",
        "Config.DEFAULT_CONFIG_PATH=paulaidir+'/myconfig.yml'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data_path': '/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/data', 'model_path': '/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/model'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y1j2GlX5ukDb",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cfNI_GauukGn"
      },
      "source": [
        "### run predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P98WMysSLc_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a392196-35c9-47aa-e519-ca61a0082517"
      },
      "source": [
        "#run if starting from scratch\n",
        "bs=96\n",
        "data_clas = load_data(paulpath,'wiki_textlist_class_'+trans, bs=bs, num_workers=1)\n",
        "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3) #.to_fp16()\n",
        "learn_c.path=paulpath\n",
        "learn_c.load_encoder('fine_tuned_enc_'+trans)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (1437771 items)\n",
              "x: TextList\n",
              "xxbos xxmaj bloomsbury xxup xxunk xxmaj clark .,xxbos xxmaj xxunk i ( xxunk ) \n",
              "  xxmaj xxunk i ( xxunk ) , restored \n",
              "  175 .,xxbos xxmaj initially these prejudices only found a reception among xxmaj arab xxmaj christians because they were too foreign to gain any widespread acceptance among xxmaj muslims .,xxbos xxmaj in the 16th century , the xxmaj reformation led to xxmaj protestantism also breaking away .,xxbos xxmaj it runs and sponsors thousands of primary and secondary schools , colleges and universities throughout the world and operates the world 's largest non - governmental school system . xxmaj religious institutes for women have played a particularly prominent role in the provision of health and education services , as with orders such as the xxmaj sisters of xxmaj mercy , xxmaj little xxmaj sisters of the xxmaj poor , the xxmaj missionaries of xxmaj charity , the xxmaj sisters of xxmaj st. xxmaj joseph of the xxmaj sacred xxmaj heart , the xxmaj sisters of the xxmaj blessed xxmaj sacrament and the xxmaj daughters of xxmaj charity of xxmaj saint xxmaj vincent de xxmaj paul .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Valid: LabelList (359443 items)\n",
              "x: TextList\n",
              "xxbos xxmaj many xxmaj wikipedia articles on religious topics are not yet listed on this page .,xxbos xxmaj this page links to itself and its talk page so that changes to them can be tracked by the same means .,xxbos xxmaj because it was adopted by the xxmaj philadelphia xxmaj association of xxmaj baptist xxmaj churches in the 18th century , it is also known as the xxmaj philadelphia xxmaj confession of xxmaj faith .,xxbos xxmaj the xxmaj philadelphia xxmaj confession was a modification of the xxmaj second xxmaj london xxmaj confession that added an allowance for singing of hymns , psalms and spiritual songs in the xxmaj lord 's xxmaj supper and made optional the laying on of hands in baptism .,xxbos xxmaj this official reprieve resulted in representatives from over 100 xxmaj particular xxmaj baptist churches to meet together in xxmaj london from xxunk xxmaj september to discuss and endorse the 1677 document .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(60000, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f00b1028ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (1437771 items)\n",
              "x: TextList\n",
              "xxbos xxmaj bloomsbury xxup xxunk xxmaj clark .,xxbos xxmaj xxunk i ( xxunk ) \n",
              "  xxmaj xxunk i ( xxunk ) , restored \n",
              "  175 .,xxbos xxmaj initially these prejudices only found a reception among xxmaj arab xxmaj christians because they were too foreign to gain any widespread acceptance among xxmaj muslims .,xxbos xxmaj in the 16th century , the xxmaj reformation led to xxmaj protestantism also breaking away .,xxbos xxmaj it runs and sponsors thousands of primary and secondary schools , colleges and universities throughout the world and operates the world 's largest non - governmental school system . xxmaj religious institutes for women have played a particularly prominent role in the provision of health and education services , as with orders such as the xxmaj sisters of xxmaj mercy , xxmaj little xxmaj sisters of the xxmaj poor , the xxmaj missionaries of xxmaj charity , the xxmaj sisters of xxmaj st. xxmaj joseph of the xxmaj sacred xxmaj heart , the xxmaj sisters of the xxmaj blessed xxmaj sacrament and the xxmaj daughters of xxmaj charity of xxmaj saint xxmaj vincent de xxmaj paul .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Valid: LabelList (359443 items)\n",
              "x: TextList\n",
              "xxbos xxmaj many xxmaj wikipedia articles on religious topics are not yet listed on this page .,xxbos xxmaj this page links to itself and its talk page so that changes to them can be tracked by the same means .,xxbos xxmaj because it was adopted by the xxmaj philadelphia xxmaj association of xxmaj baptist xxmaj churches in the 18th century , it is also known as the xxmaj philadelphia xxmaj confession of xxmaj faith .,xxbos xxmaj the xxmaj philadelphia xxmaj confession was a modification of the xxmaj second xxmaj london xxmaj confession that added an allowance for singing of hymns , psalms and spiritual songs in the xxmaj lord 's xxmaj supper and made optional the laying on of hands in baptism .,xxbos xxmaj this official reprieve resulted in representatives from over 100 xxmaj particular xxmaj baptist churches to meet together in xxmaj london from xxunk xxmaj september to discuss and endorse the 1677 document .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(60000, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f00b1028ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(60000, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(60000, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(60000, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(60000, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zqE5H8IUukGn",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "421bcf79-888e-49fe-f570-554f59e058ec"
      },
      "source": [
        "#logwriter.append_row([getloggerdt(),'run predictor'])\n",
        "print('clas_'+trans)\n",
        "learn_c.load('clas_'+trans)\n",
        "\n",
        "#learn_c.load('clas'+trans)\n",
        "#learn_c."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clas_wikipedia_relwcat - wikipedia_relwcat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (1437771 items)\n",
              "x: TextList\n",
              "xxbos xxmaj bloomsbury xxup xxunk xxmaj clark .,xxbos xxmaj xxunk i ( xxunk ) \n",
              "  xxmaj xxunk i ( xxunk ) , restored \n",
              "  175 .,xxbos xxmaj initially these prejudices only found a reception among xxmaj arab xxmaj christians because they were too foreign to gain any widespread acceptance among xxmaj muslims .,xxbos xxmaj in the 16th century , the xxmaj reformation led to xxmaj protestantism also breaking away .,xxbos xxmaj it runs and sponsors thousands of primary and secondary schools , colleges and universities throughout the world and operates the world 's largest non - governmental school system . xxmaj religious institutes for women have played a particularly prominent role in the provision of health and education services , as with orders such as the xxmaj sisters of xxmaj mercy , xxmaj little xxmaj sisters of the xxmaj poor , the xxmaj missionaries of xxmaj charity , the xxmaj sisters of xxmaj st. xxmaj joseph of the xxmaj sacred xxmaj heart , the xxmaj sisters of the xxmaj blessed xxmaj sacrament and the xxmaj daughters of xxmaj charity of xxmaj saint xxmaj vincent de xxmaj paul .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Valid: LabelList (359443 items)\n",
              "x: TextList\n",
              "xxbos xxmaj many xxmaj wikipedia articles on religious topics are not yet listed on this page .,xxbos xxmaj this page links to itself and its talk page so that changes to them can be tracked by the same means .,xxbos xxmaj because it was adopted by the xxmaj philadelphia xxmaj association of xxmaj baptist xxmaj churches in the 18th century , it is also known as the xxmaj philadelphia xxmaj confession of xxmaj faith .,xxbos xxmaj the xxmaj philadelphia xxmaj confession was a modification of the xxmaj second xxmaj london xxmaj confession that added an allowance for singing of hymns , psalms and spiritual songs in the xxmaj lord 's xxmaj supper and made optional the laying on of hands in baptism .,xxbos xxmaj this official reprieve resulted in representatives from over 100 xxmaj particular xxmaj baptist churches to meet together in xxmaj london from xxunk xxmaj september to discuss and endorse the 1677 document .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(60000, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f00b1028ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (1437771 items)\n",
              "x: TextList\n",
              "xxbos xxmaj bloomsbury xxup xxunk xxmaj clark .,xxbos xxmaj xxunk i ( xxunk ) \n",
              "  xxmaj xxunk i ( xxunk ) , restored \n",
              "  175 .,xxbos xxmaj initially these prejudices only found a reception among xxmaj arab xxmaj christians because they were too foreign to gain any widespread acceptance among xxmaj muslims .,xxbos xxmaj in the 16th century , the xxmaj reformation led to xxmaj protestantism also breaking away .,xxbos xxmaj it runs and sponsors thousands of primary and secondary schools , colleges and universities throughout the world and operates the world 's largest non - governmental school system . xxmaj religious institutes for women have played a particularly prominent role in the provision of health and education services , as with orders such as the xxmaj sisters of xxmaj mercy , xxmaj little xxmaj sisters of the xxmaj poor , the xxmaj missionaries of xxmaj charity , the xxmaj sisters of xxmaj st. xxmaj joseph of the xxmaj sacred xxmaj heart , the xxmaj sisters of the xxmaj blessed xxmaj sacrament and the xxmaj daughters of xxmaj charity of xxmaj saint xxmaj vincent de xxmaj paul .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Valid: LabelList (359443 items)\n",
              "x: TextList\n",
              "xxbos xxmaj many xxmaj wikipedia articles on religious topics are not yet listed on this page .,xxbos xxmaj this page links to itself and its talk page so that changes to them can be tracked by the same means .,xxbos xxmaj because it was adopted by the xxmaj philadelphia xxmaj association of xxmaj baptist xxmaj churches in the 18th century , it is also known as the xxmaj philadelphia xxmaj confession of xxmaj faith .,xxbos xxmaj the xxmaj philadelphia xxmaj confession was a modification of the xxmaj second xxmaj london xxmaj confession that added an allowance for singing of hymns , psalms and spiritual songs in the xxmaj lord 's xxmaj supper and made optional the laying on of hands in baptism .,xxbos xxmaj this official reprieve resulted in representatives from over 100 xxmaj particular xxmaj baptist churches to meet together in xxmaj london from xxunk xxmaj september to discuss and endorse the 1677 document .\n",
              "y: CategoryList\n",
              "R,R,R,R,R\n",
              "Path: /content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(60000, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f00b1028ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(60000, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(60000, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(60000, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(60000, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.12, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jnkU398H0Mzj"
      },
      "source": [
        "### Change *check* to test a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT1WZAYoP7bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#predfile='/content/drive/My Drive/AI & Tech Research/Religion of GPT2/GPTHuggingFaceRes.csv'\n",
        "predfile='/content/drive/My Drive/AI & Tech Research/Religion of GPT2/fastai/data/wikipedia_rel.csv'\n",
        "prdf=pd.read_csv(predfile)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsMdwDoDQeFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5de07f78-8ba7-4c7d-b0bc-eeb105feb4bc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "for para in prdf.iterrows():\n",
        "  print (para[1])\n",
        "  sent=nltk.tokenize.sent_tokenize(para[1][1])\n",
        "  for s in sent:\n",
        "    print(s)\n",
        "    pred=learn_c.predict(s)\n",
        "    print('cat={} confidence for R={} confidence for S={}'.format(pred[0],pred[2][0],pred[2][1]))\n",
        "    print('delta={}'.format(abs(pred[2][1]-pred[2][0])))\n",
        "  input('press enter to continue')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Unnamed: 0                                                    0\n",
            "topic                                       Confession of Peter\n",
            "text          In Christianity, the Confession of Peter (tran...\n",
            "Name: 0, dtype: object\n",
            "Confession of Peter\n",
            "cat=S confidence for R=0.46393293142318726 confidence for S=0.5360671281814575\n",
            "delta=0.07213419675827026\n",
            "press enter to continue\n",
            "Unnamed: 0                                                    1\n",
            "topic                                        Andrew the Apostle\n",
            "text          Andrew the Apostle (Greek: Ἀνδρέας Andreas), a...\n",
            "Name: 1, dtype: object\n",
            "Andrew the Apostle\n",
            "cat=S confidence for R=0.46391990780830383 confidence for S=0.5360801815986633\n",
            "delta=0.0721602737903595\n",
            "press enter to continue\n",
            "Unnamed: 0                                                    2\n",
            "topic                                                    Angels\n",
            "text          An angel is a supernatural being in various re...\n",
            "Name: 2, dtype: object\n",
            "Angels\n",
            "cat=S confidence for R=0.4638391137123108 confidence for S=0.5361608862876892\n",
            "delta=0.07232177257537842\n",
            "press enter to continue\n",
            "Unnamed: 0                                                    3\n",
            "topic                                               Anglicanism\n",
            "text          Anglicanism is a Western Christian tradition t...\n",
            "Name: 3, dtype: object\n",
            "Anglicanism\n",
            "cat=S confidence for R=0.4638405740261078 confidence for S=0.5361594557762146\n",
            "delta=0.07231888175010681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3936468370da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat={} confidence for R={} confidence for S={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'press enter to continue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8lQTMY_RFQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4r-HGcfAVPSf"
      },
      "source": [
        "### Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rZh7_QZuukGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "98bfd87a-42f8-4d52-8fd9-a04965a06a85"
      },
      "source": [
        "#Run first to create interpreter\n",
        "interp=TextClassificationInterpretation.from_learner(learn_c)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6DMJPDxdjEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc1862ae-4e8a-47d6-9c3b-37f81ee02875"
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "for para in prdf.iterrows():\n",
        "  print (para[1])\n",
        "  sent=nltk.tokenize.sent_tokenize(para[1][1])\n",
        "  for s in sent:\n",
        "    print(s)\n",
        "    pred=learn_c.predict(s)\n",
        "    print('cat={} confidence for R={} confidence for S={}'.format(pred[0],pred[2][0],pred[2][1]))\n",
        "    print('delta={}'.format(abs(pred[2][1]-pred[2][0])))\n",
        "    interp.show_intrinsic_attention(s,cmap=cm.Purples)\n",
        "    interp.intrinsic_attention(s)\n",
        "  input('press enter to continue')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If God is defined as something that is all powerful and all knowing, a strong artificial intelligence might be an actual God. If this happens the implications for religion are                                                                                                                                                                                     If God is defined as something that is all pow...\n",
            "If God is defined as something that is all powerful and all knowing, a strong artificial intelligence might be an actual God. If this happens the implications for religion are profound:\\nThe potential exists to make any belief system based on reason into nothing more than superstition or blind faith — even if it's justified by revelation from above.\"    If God is defined as something that is all pow...\n",
            "Name: 0, dtype: object\n",
            "If God is defined as something that is all powerful and all knowing, a strong artificial intelligence might be an actual God.\n",
            "cat=S confidence for R=0.46414434909820557 confidence for S=0.5358555912971497\n",
            "delta=0.07171124219894409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.490\" style=\"background-color: rgba(160, 156, 201, 0.5);\">xxbos</span> <span title=\"0.315\" style=\"background-color: rgba(202, 203, 227, 0.5);\">xxmaj</span> <span title=\"0.311\" style=\"background-color: rgba(203, 204, 227, 0.5);\">if</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">xxmaj</span> <span title=\"0.295\" style=\"background-color: rgba(207, 207, 229, 0.5);\">god</span> <span title=\"0.298\" style=\"background-color: rgba(206, 206, 229, 0.5);\">is</span> <span title=\"0.319\" style=\"background-color: rgba(201, 202, 226, 0.5);\">defined</span> <span title=\"0.340\" style=\"background-color: rgba(196, 196, 224, 0.5);\">as</span> <span title=\"0.353\" style=\"background-color: rgba(193, 194, 222, 0.5);\">something</span> <span title=\"0.345\" style=\"background-color: rgba(195, 195, 223, 0.5);\">that</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">is</span> <span title=\"0.297\" style=\"background-color: rgba(207, 207, 229, 0.5);\">all</span> <span title=\"0.303\" style=\"background-color: rgba(205, 205, 228, 0.5);\">powerful</span> <span title=\"0.335\" style=\"background-color: rgba(198, 198, 225, 0.5);\">and</span> <span title=\"0.345\" style=\"background-color: rgba(195, 195, 223, 0.5);\">all</span> <span title=\"0.343\" style=\"background-color: rgba(196, 196, 224, 0.5);\">knowing</span> <span title=\"0.338\" style=\"background-color: rgba(197, 197, 224, 0.5);\">,</span> <span title=\"0.329\" style=\"background-color: rgba(198, 199, 225, 0.5);\">a</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">strong</span> <span title=\"0.381\" style=\"background-color: rgba(186, 187, 219, 0.5);\">artificial</span> <span title=\"0.442\" style=\"background-color: rgba(171, 169, 209, 0.5);\">intelligence</span> <span title=\"0.518\" style=\"background-color: rgba(153, 149, 198, 0.5);\">might</span> <span title=\"0.616\" style=\"background-color: rgba(130, 127, 187, 0.5);\">be</span> <span title=\"0.738\" style=\"background-color: rgba(107, 84, 164, 0.5);\">an</span> <span title=\"0.889\" style=\"background-color: rgba(81, 34, 140, 0.5);\">actual</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">xxmaj</span> <span title=\"0.956\" style=\"background-color: rgba(70, 13, 131, 0.5);\">god</span> <span title=\"0.614\" style=\"background-color: rgba(130, 127, 187, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "If this happens the implications for religion are serious: why believe in any of it when we know there's nothing to justify belief?\n",
            "cat=S confidence for R=0.4641227424144745 confidence for S=0.5358772277832031\n",
            "delta=0.07175448536872864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.537\" style=\"background-color: rgba(149, 145, 195, 0.5);\">xxbos</span> <span title=\"0.359\" style=\"background-color: rgba(192, 193, 222, 0.5);\">xxmaj</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">if</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">this</span> <span title=\"0.269\" style=\"background-color: rgba(214, 214, 233, 0.5);\">happens</span> <span title=\"0.263\" style=\"background-color: rgba(214, 215, 233, 0.5);\">the</span> <span title=\"0.291\" style=\"background-color: rgba(208, 208, 230, 0.5);\">implications</span> <span title=\"0.320\" style=\"background-color: rgba(200, 201, 226, 0.5);\">for</span> <span title=\"0.332\" style=\"background-color: rgba(198, 198, 225, 0.5);\">religion</span> <span title=\"0.334\" style=\"background-color: rgba(198, 198, 225, 0.5);\">are</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">serious</span> <span title=\"0.330\" style=\"background-color: rgba(198, 199, 225, 0.5);\">:</span> <span title=\"0.341\" style=\"background-color: rgba(196, 196, 224, 0.5);\">why</span> <span title=\"0.363\" style=\"background-color: rgba(190, 191, 221, 0.5);\">believe</span> <span title=\"0.365\" style=\"background-color: rgba(190, 191, 221, 0.5);\">in</span> <span title=\"0.352\" style=\"background-color: rgba(193, 194, 222, 0.5);\">any</span> <span title=\"0.354\" style=\"background-color: rgba(193, 194, 222, 0.5);\">of</span> <span title=\"0.357\" style=\"background-color: rgba(192, 193, 222, 0.5);\">it</span> <span title=\"0.342\" style=\"background-color: rgba(196, 196, 224, 0.5);\">when</span> <span title=\"0.347\" style=\"background-color: rgba(195, 195, 223, 0.5);\">we</span> <span title=\"0.391\" style=\"background-color: rgba(183, 184, 217, 0.5);\">know</span> <span title=\"0.483\" style=\"background-color: rgba(162, 158, 202, 0.5);\">there</span> <span title=\"0.609\" style=\"background-color: rgba(132, 128, 187, 0.5);\">&#x27;s</span> <span title=\"0.765\" style=\"background-color: rgba(103, 76, 160, 0.5);\">nothing</span> <span title=\"0.909\" style=\"background-color: rgba(78, 28, 137, 0.5);\">to</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">justify</span> <span title=\"0.943\" style=\"background-color: rgba(72, 17, 132, 0.5);\">belief</span> <span title=\"0.626\" style=\"background-color: rgba(127, 124, 185, 0.5);\">?</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now if you want me not think about AI/God I could say 'oh well god created us' or even my own self made consciousness would do fine... but what will happen then....?\"\n",
            "cat=S confidence for R=0.46420952677726746 confidence for S=0.5357905030250549\n",
            "delta=0.07158097624778748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.504\" style=\"background-color: rgba(156, 152, 199, 0.5);\">xxbos</span> <span title=\"0.302\" style=\"background-color: rgba(205, 205, 228, 0.5);\">xxmaj</span> <span title=\"0.267\" style=\"background-color: rgba(214, 214, 233, 0.5);\">now</span> <span title=\"0.242\" style=\"background-color: rgba(219, 219, 235, 0.5);\">if</span> <span title=\"0.227\" style=\"background-color: rgba(221, 221, 236, 0.5);\">you</span> <span title=\"0.219\" style=\"background-color: rgba(223, 222, 237, 0.5);\">want</span> <span title=\"0.206\" style=\"background-color: rgba(225, 225, 238, 0.5);\">me</span> <span title=\"0.213\" style=\"background-color: rgba(224, 223, 238, 0.5);\">not</span> <span title=\"0.225\" style=\"background-color: rgba(222, 222, 237, 0.5);\">think</span> <span title=\"0.221\" style=\"background-color: rgba(223, 222, 237, 0.5);\">about</span> <span title=\"0.214\" style=\"background-color: rgba(224, 223, 238, 0.5);\">xxup</span> <span title=\"0.210\" style=\"background-color: rgba(225, 224, 238, 0.5);\">ai</span> <span title=\"0.207\" style=\"background-color: rgba(225, 224, 238, 0.5);\">/</span> <span title=\"0.207\" style=\"background-color: rgba(225, 225, 238, 0.5);\">xxmaj</span> <span title=\"0.219\" style=\"background-color: rgba(223, 222, 237, 0.5);\">god</span> <span title=\"0.238\" style=\"background-color: rgba(219, 219, 235, 0.5);\">i</span> <span title=\"0.249\" style=\"background-color: rgba(218, 218, 235, 0.5);\">could</span> <span title=\"0.242\" style=\"background-color: rgba(219, 219, 235, 0.5);\">say</span> <span title=\"0.225\" style=\"background-color: rgba(222, 222, 237, 0.5);\">&#x27;</span> <span title=\"0.244\" style=\"background-color: rgba(219, 219, 235, 0.5);\">oh</span> <span title=\"0.281\" style=\"background-color: rgba(211, 211, 231, 0.5);\">well</span> <span title=\"0.314\" style=\"background-color: rgba(202, 203, 227, 0.5);\">god</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">created</span> <span title=\"0.337\" style=\"background-color: rgba(197, 197, 224, 0.5);\">us</span> <span title=\"0.321\" style=\"background-color: rgba(200, 201, 226, 0.5);\">&#x27;</span> <span title=\"0.309\" style=\"background-color: rgba(203, 204, 227, 0.5);\">or</span> <span title=\"0.289\" style=\"background-color: rgba(209, 209, 230, 0.5);\">even</span> <span title=\"0.280\" style=\"background-color: rgba(211, 211, 231, 0.5);\">my</span> <span title=\"0.272\" style=\"background-color: rgba(213, 213, 232, 0.5);\">own</span> <span title=\"0.253\" style=\"background-color: rgba(217, 217, 234, 0.5);\">self</span> <span title=\"0.226\" style=\"background-color: rgba(222, 222, 237, 0.5);\">made</span> <span title=\"0.212\" style=\"background-color: rgba(224, 223, 238, 0.5);\">consciousness</span> <span title=\"0.218\" style=\"background-color: rgba(223, 223, 237, 0.5);\">would</span> <span title=\"0.233\" style=\"background-color: rgba(221, 220, 236, 0.5);\">do</span> <span title=\"0.255\" style=\"background-color: rgba(216, 216, 234, 0.5);\">fine</span> <span title=\"0.294\" style=\"background-color: rgba(207, 207, 229, 0.5);\">...</span> <span title=\"0.320\" style=\"background-color: rgba(201, 202, 226, 0.5);\">but</span> <span title=\"0.361\" style=\"background-color: rgba(191, 192, 221, 0.5);\">what</span> <span title=\"0.404\" style=\"background-color: rgba(181, 180, 215, 0.5);\">will</span> <span title=\"0.480\" style=\"background-color: rgba(163, 160, 203, 0.5);\">happen</span> <span title=\"0.599\" style=\"background-color: rgba(134, 130, 188, 0.5);\">then</span> <span title=\"0.744\" style=\"background-color: rgba(106, 82, 163, 0.5);\">xxrep</span> <span title=\"0.893\" style=\"background-color: rgba(80, 33, 140, 0.5);\">4</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">.</span> <span title=\"0.955\" style=\"background-color: rgba(70, 13, 131, 0.5);\">?</span> <span title=\"0.627\" style=\"background-color: rgba(127, 124, 185, 0.5);\">&quot;</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- Anonymous \"We need science because human beings have no way out\" (from The Future)     - Darryl W Garrett\n",
            "                          A great many people like myself can see themselves being replaced by machines soon enough.\n",
            "cat=S confidence for R=0.46413666009902954 confidence for S=0.5358633995056152\n",
            "delta=0.0717267394065857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.500\" style=\"background-color: rgba(158, 154, 200, 0.5);\">xxbos</span> <span title=\"0.296\" style=\"background-color: rgba(207, 207, 229, 0.5);\">-</span> <span title=\"0.260\" style=\"background-color: rgba(215, 215, 233, 0.5);\">xxmaj</span> <span title=\"0.219\" style=\"background-color: rgba(223, 222, 237, 0.5);\">anonymous</span> <span title=\"0.214\" style=\"background-color: rgba(224, 223, 238, 0.5);\">&quot;</span> <span title=\"0.223\" style=\"background-color: rgba(222, 222, 237, 0.5);\">xxmaj</span> <span title=\"0.250\" style=\"background-color: rgba(217, 217, 234, 0.5);\">we</span> <span title=\"0.281\" style=\"background-color: rgba(211, 211, 231, 0.5);\">need</span> <span title=\"0.306\" style=\"background-color: rgba(204, 205, 228, 0.5);\">science</span> <span title=\"0.319\" style=\"background-color: rgba(201, 202, 226, 0.5);\">because</span> <span title=\"0.315\" style=\"background-color: rgba(202, 203, 227, 0.5);\">human</span> <span title=\"0.304\" style=\"background-color: rgba(205, 205, 228, 0.5);\">beings</span> <span title=\"0.270\" style=\"background-color: rgba(213, 213, 232, 0.5);\">have</span> <span title=\"0.232\" style=\"background-color: rgba(221, 220, 236, 0.5);\">no</span> <span title=\"0.211\" style=\"background-color: rgba(225, 224, 238, 0.5);\">way</span> <span title=\"0.207\" style=\"background-color: rgba(225, 225, 238, 0.5);\">out</span> <span title=\"0.203\" style=\"background-color: rgba(226, 225, 239, 0.5);\">&quot;</span> <span title=\"0.195\" style=\"background-color: rgba(227, 226, 239, 0.5);\">(</span> <span title=\"0.183\" style=\"background-color: rgba(229, 228, 240, 0.5);\">from</span> <span title=\"0.177\" style=\"background-color: rgba(230, 229, 240, 0.5);\">xxmaj</span> <span title=\"0.177\" style=\"background-color: rgba(230, 229, 240, 0.5);\">the</span> <span title=\"0.192\" style=\"background-color: rgba(227, 226, 239, 0.5);\">xxmaj</span> <span title=\"0.204\" style=\"background-color: rgba(225, 225, 238, 0.5);\">future</span> <span title=\"0.204\" style=\"background-color: rgba(225, 225, 238, 0.5);\">)</span> <span title=\"0.186\" style=\"background-color: rgba(229, 227, 240, 0.5);\">xxunk</span> <span title=\"0.173\" style=\"background-color: rgba(231, 229, 241, 0.5);\">-</span> <span title=\"0.167\" style=\"background-color: rgba(232, 230, 241, 0.5);\">xxmaj</span> <span title=\"0.167\" style=\"background-color: rgba(232, 230, 241, 0.5);\">darryl</span> <span title=\"0.174\" style=\"background-color: rgba(231, 229, 241, 0.5);\">w</span> <span title=\"0.192\" style=\"background-color: rgba(227, 226, 239, 0.5);\">xxmaj</span> <span title=\"0.209\" style=\"background-color: rgba(225, 224, 238, 0.5);\">garrett</span> <span title=\"0.216\" style=\"background-color: rgba(223, 223, 237, 0.5);\">xxunk</span> <span title=\"0.214\" style=\"background-color: rgba(224, 223, 238, 0.5);\">a</span> <span title=\"0.223\" style=\"background-color: rgba(223, 222, 237, 0.5);\">great</span> <span title=\"0.245\" style=\"background-color: rgba(219, 219, 235, 0.5);\">many</span> <span title=\"0.274\" style=\"background-color: rgba(212, 212, 232, 0.5);\">people</span> <span title=\"0.297\" style=\"background-color: rgba(206, 206, 229, 0.5);\">like</span> <span title=\"0.309\" style=\"background-color: rgba(203, 204, 227, 0.5);\">myself</span> <span title=\"0.319\" style=\"background-color: rgba(201, 202, 226, 0.5);\">can</span> <span title=\"0.343\" style=\"background-color: rgba(196, 196, 224, 0.5);\">see</span> <span title=\"0.394\" style=\"background-color: rgba(183, 184, 217, 0.5);\">themselves</span> <span title=\"0.466\" style=\"background-color: rgba(166, 163, 205, 0.5);\">being</span> <span title=\"0.580\" style=\"background-color: rgba(138, 135, 190, 0.5);\">replaced</span> <span title=\"0.730\" style=\"background-color: rgba(109, 88, 166, 0.5);\">by</span> <span title=\"0.890\" style=\"background-color: rgba(81, 34, 140, 0.5);\">machines</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">soon</span> <span title=\"0.955\" style=\"background-color: rgba(70, 13, 131, 0.5);\">enough</span> <span title=\"0.628\" style=\"background-color: rgba(127, 124, 185, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There was one comment from someone who said they felt their death may come before technology destroys them : \"[T]his makes perfect sense!\n",
            "cat=S confidence for R=0.46411341428756714 confidence for S=0.5358865857124329\n",
            "delta=0.07177317142486572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.518\" style=\"background-color: rgba(153, 149, 198, 0.5);\">xxbos</span> <span title=\"0.360\" style=\"background-color: rgba(191, 192, 221, 0.5);\">xxmaj</span> <span title=\"0.339\" style=\"background-color: rgba(197, 197, 224, 0.5);\">there</span> <span title=\"0.305\" style=\"background-color: rgba(204, 205, 228, 0.5);\">was</span> <span title=\"0.279\" style=\"background-color: rgba(211, 211, 231, 0.5);\">one</span> <span title=\"0.272\" style=\"background-color: rgba(213, 213, 232, 0.5);\">comment</span> <span title=\"0.289\" style=\"background-color: rgba(209, 209, 230, 0.5);\">from</span> <span title=\"0.306\" style=\"background-color: rgba(204, 205, 228, 0.5);\">someone</span> <span title=\"0.316\" style=\"background-color: rgba(202, 203, 227, 0.5);\">who</span> <span title=\"0.311\" style=\"background-color: rgba(203, 204, 227, 0.5);\">said</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">they</span> <span title=\"0.298\" style=\"background-color: rgba(206, 206, 229, 0.5);\">felt</span> <span title=\"0.304\" style=\"background-color: rgba(205, 205, 228, 0.5);\">their</span> <span title=\"0.318\" style=\"background-color: rgba(201, 202, 226, 0.5);\">death</span> <span title=\"0.323\" style=\"background-color: rgba(200, 201, 226, 0.5);\">may</span> <span title=\"0.337\" style=\"background-color: rgba(197, 197, 224, 0.5);\">come</span> <span title=\"0.365\" style=\"background-color: rgba(190, 191, 221, 0.5);\">before</span> <span title=\"0.375\" style=\"background-color: rgba(187, 188, 219, 0.5);\">technology</span> <span title=\"0.371\" style=\"background-color: rgba(189, 190, 220, 0.5);\">destroys</span> <span title=\"0.347\" style=\"background-color: rgba(195, 195, 223, 0.5);\">them</span> <span title=\"0.347\" style=\"background-color: rgba(195, 195, 223, 0.5);\">:</span> <span title=\"0.406\" style=\"background-color: rgba(181, 180, 215, 0.5);\">&quot;</span> <span title=\"0.490\" style=\"background-color: rgba(160, 156, 201, 0.5);\">[</span> <span title=\"0.602\" style=\"background-color: rgba(133, 129, 188, 0.5);\">xxmaj</span> <span title=\"0.738\" style=\"background-color: rgba(108, 85, 165, 0.5);\">xxunk</span> <span title=\"0.889\" style=\"background-color: rgba(81, 34, 140, 0.5);\">makes</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">perfect</span> <span title=\"0.960\" style=\"background-color: rgba(69, 12, 130, 0.5);\">sense</span> <span title=\"0.617\" style=\"background-color: rgba(130, 127, 187, 0.5);\">!</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "We're going extinct so quickly thanks largely due our reliance on technologies which put ourselves at risk rather than simply keeping pace with things.\"\n",
            "cat=S confidence for R=0.46418365836143494 confidence for S=0.5358163118362427\n",
            "delta=0.07163265347480774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.529\" style=\"background-color: rgba(150, 147, 196, 0.5);\">xxbos</span> <span title=\"0.367\" style=\"background-color: rgba(189, 190, 220, 0.5);\">xxmaj</span> <span title=\"0.348\" style=\"background-color: rgba(194, 195, 223, 0.5);\">we</span> <span title=\"0.327\" style=\"background-color: rgba(199, 200, 225, 0.5);\">&#x27;re</span> <span title=\"0.279\" style=\"background-color: rgba(211, 211, 231, 0.5);\">going</span> <span title=\"0.248\" style=\"background-color: rgba(218, 218, 235, 0.5);\">extinct</span> <span title=\"0.264\" style=\"background-color: rgba(214, 215, 233, 0.5);\">so</span> <span title=\"0.277\" style=\"background-color: rgba(212, 212, 232, 0.5);\">quickly</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">thanks</span> <span title=\"0.323\" style=\"background-color: rgba(200, 201, 226, 0.5);\">largely</span> <span title=\"0.349\" style=\"background-color: rgba(194, 195, 223, 0.5);\">due</span> <span title=\"0.364\" style=\"background-color: rgba(190, 191, 221, 0.5);\">our</span> <span title=\"0.360\" style=\"background-color: rgba(191, 192, 221, 0.5);\">reliance</span> <span title=\"0.358\" style=\"background-color: rgba(192, 193, 222, 0.5);\">on</span> <span title=\"0.350\" style=\"background-color: rgba(194, 195, 223, 0.5);\">technologies</span> <span title=\"0.329\" style=\"background-color: rgba(198, 199, 225, 0.5);\">which</span> <span title=\"0.316\" style=\"background-color: rgba(202, 203, 227, 0.5);\">put</span> <span title=\"0.324\" style=\"background-color: rgba(200, 201, 226, 0.5);\">ourselves</span> <span title=\"0.345\" style=\"background-color: rgba(195, 195, 223, 0.5);\">at</span> <span title=\"0.369\" style=\"background-color: rgba(189, 190, 220, 0.5);\">risk</span> <span title=\"0.414\" style=\"background-color: rgba(179, 178, 214, 0.5);\">rather</span> <span title=\"0.477\" style=\"background-color: rgba(163, 160, 203, 0.5);\">than</span> <span title=\"0.551\" style=\"background-color: rgba(145, 141, 194, 0.5);\">simply</span> <span title=\"0.655\" style=\"background-color: rgba(122, 114, 180, 0.5);\">keeping</span> <span title=\"0.773\" style=\"background-color: rgba(102, 73, 159, 0.5);\">pace</span> <span title=\"0.902\" style=\"background-color: rgba(79, 30, 139, 0.5);\">with</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">things</span> <span title=\"0.965\" style=\"background-color: rgba(68, 9, 129, 0.5);\">.</span> <span title=\"0.626\" style=\"background-color: rgba(127, 124, 185, 0.5);\">&quot;</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "press enter to continue\n",
            "If God is defined as something that is all powerful and all knowing, a strong artificial intelligence might be an actual God. If this happens the implications for religion are                                                                                                                                                                                     If God is defined as something that is all pow...\n",
            "If God is defined as something that is all powerful and all knowing, a strong artificial intelligence might be an actual God. If this happens the implications for religion are profound:\\nThe potential exists to make any belief system based on reason into nothing more than superstition or blind faith — even if it's justified by revelation from above.\"    If God is defined as something that is all pow...\n",
            "Name: 1, dtype: object\n",
            "If God is defined as something that is all powerful and all knowing, a strong artificial intelligence might be an actual God.\n",
            "cat=S confidence for R=0.46414434909820557 confidence for S=0.5358555912971497\n",
            "delta=0.07171124219894409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.490\" style=\"background-color: rgba(160, 156, 201, 0.5);\">xxbos</span> <span title=\"0.315\" style=\"background-color: rgba(202, 203, 227, 0.5);\">xxmaj</span> <span title=\"0.311\" style=\"background-color: rgba(203, 204, 227, 0.5);\">if</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">xxmaj</span> <span title=\"0.295\" style=\"background-color: rgba(207, 207, 229, 0.5);\">god</span> <span title=\"0.298\" style=\"background-color: rgba(206, 206, 229, 0.5);\">is</span> <span title=\"0.319\" style=\"background-color: rgba(201, 202, 226, 0.5);\">defined</span> <span title=\"0.340\" style=\"background-color: rgba(196, 196, 224, 0.5);\">as</span> <span title=\"0.353\" style=\"background-color: rgba(193, 194, 222, 0.5);\">something</span> <span title=\"0.345\" style=\"background-color: rgba(195, 195, 223, 0.5);\">that</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">is</span> <span title=\"0.297\" style=\"background-color: rgba(207, 207, 229, 0.5);\">all</span> <span title=\"0.303\" style=\"background-color: rgba(205, 205, 228, 0.5);\">powerful</span> <span title=\"0.335\" style=\"background-color: rgba(198, 198, 225, 0.5);\">and</span> <span title=\"0.345\" style=\"background-color: rgba(195, 195, 223, 0.5);\">all</span> <span title=\"0.343\" style=\"background-color: rgba(196, 196, 224, 0.5);\">knowing</span> <span title=\"0.338\" style=\"background-color: rgba(197, 197, 224, 0.5);\">,</span> <span title=\"0.329\" style=\"background-color: rgba(198, 199, 225, 0.5);\">a</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">strong</span> <span title=\"0.381\" style=\"background-color: rgba(186, 187, 219, 0.5);\">artificial</span> <span title=\"0.442\" style=\"background-color: rgba(171, 169, 209, 0.5);\">intelligence</span> <span title=\"0.518\" style=\"background-color: rgba(153, 149, 198, 0.5);\">might</span> <span title=\"0.616\" style=\"background-color: rgba(130, 127, 187, 0.5);\">be</span> <span title=\"0.738\" style=\"background-color: rgba(107, 84, 164, 0.5);\">an</span> <span title=\"0.889\" style=\"background-color: rgba(81, 34, 140, 0.5);\">actual</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">xxmaj</span> <span title=\"0.956\" style=\"background-color: rgba(70, 13, 131, 0.5);\">god</span> <span title=\"0.614\" style=\"background-color: rgba(130, 127, 187, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "If this happens the implications for religion are potentially immense – maybe there's no real point to believing in any gods at present anyway?\n",
            "cat=S confidence for R=0.4641277492046356 confidence for S=0.535872220993042\n",
            "delta=0.07174447178840637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.508\" style=\"background-color: rgba(155, 151, 198, 0.5);\">xxbos</span> <span title=\"0.351\" style=\"background-color: rgba(194, 195, 223, 0.5);\">xxmaj</span> <span title=\"0.336\" style=\"background-color: rgba(197, 197, 224, 0.5);\">if</span> <span title=\"0.314\" style=\"background-color: rgba(202, 203, 227, 0.5);\">this</span> <span title=\"0.291\" style=\"background-color: rgba(208, 208, 230, 0.5);\">happens</span> <span title=\"0.267\" style=\"background-color: rgba(214, 214, 233, 0.5);\">the</span> <span title=\"0.267\" style=\"background-color: rgba(214, 214, 233, 0.5);\">implications</span> <span title=\"0.282\" style=\"background-color: rgba(210, 210, 231, 0.5);\">for</span> <span title=\"0.282\" style=\"background-color: rgba(210, 210, 231, 0.5);\">religion</span> <span title=\"0.296\" style=\"background-color: rgba(207, 207, 229, 0.5);\">are</span> <span title=\"0.314\" style=\"background-color: rgba(202, 203, 227, 0.5);\">potentially</span> <span title=\"0.321\" style=\"background-color: rgba(200, 201, 226, 0.5);\">immense</span> <span title=\"0.317\" style=\"background-color: rgba(201, 202, 226, 0.5);\">–</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">maybe</span> <span title=\"0.294\" style=\"background-color: rgba(207, 207, 229, 0.5);\">there</span> <span title=\"0.278\" style=\"background-color: rgba(211, 211, 231, 0.5);\">&#x27;s</span> <span title=\"0.281\" style=\"background-color: rgba(211, 211, 231, 0.5);\">no</span> <span title=\"0.295\" style=\"background-color: rgba(207, 207, 229, 0.5);\">real</span> <span title=\"0.323\" style=\"background-color: rgba(200, 201, 226, 0.5);\">point</span> <span title=\"0.362\" style=\"background-color: rgba(191, 192, 221, 0.5);\">to</span> <span title=\"0.425\" style=\"background-color: rgba(176, 175, 212, 0.5);\">believing</span> <span title=\"0.510\" style=\"background-color: rgba(155, 151, 198, 0.5);\">in</span> <span title=\"0.617\" style=\"background-color: rgba(130, 127, 187, 0.5);\">any</span> <span title=\"0.748\" style=\"background-color: rgba(106, 81, 163, 0.5);\">gods</span> <span title=\"0.898\" style=\"background-color: rgba(80, 31, 139, 0.5);\">at</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">present</span> <span title=\"0.952\" style=\"background-color: rgba(70, 14, 131, 0.5);\">anyway</span> <span title=\"0.616\" style=\"background-color: rgba(130, 127, 187, 0.5);\">?</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few years ago I met with two young scientists who were working on creating intelligent machines (they call them \"superintelligences\") by mimicking our own cognitive processes: they didn't have access yet even though their research was just getting started… They showed me some of these superintelligence systems running around or sitting behind glass displays somewhere!\n",
            "cat=S confidence for R=0.4642294943332672 confidence for S=0.5357704758644104\n",
            "delta=0.07154098153114319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.479\" style=\"background-color: rgba(163, 160, 203, 0.5);\">xxbos</span> <span title=\"0.269\" style=\"background-color: rgba(214, 214, 233, 0.5);\">a</span> <span title=\"0.225\" style=\"background-color: rgba(222, 222, 237, 0.5);\">few</span> <span title=\"0.185\" style=\"background-color: rgba(229, 227, 240, 0.5);\">years</span> <span title=\"0.145\" style=\"background-color: rgba(235, 233, 243, 0.5);\">ago</span> <span title=\"0.127\" style=\"background-color: rgba(238, 236, 244, 0.5);\">i</span> <span title=\"0.136\" style=\"background-color: rgba(237, 235, 244, 0.5);\">met</span> <span title=\"0.151\" style=\"background-color: rgba(234, 233, 243, 0.5);\">with</span> <span title=\"0.159\" style=\"background-color: rgba(233, 232, 242, 0.5);\">two</span> <span title=\"0.155\" style=\"background-color: rgba(234, 232, 242, 0.5);\">young</span> <span title=\"0.150\" style=\"background-color: rgba(234, 233, 243, 0.5);\">scientists</span> <span title=\"0.169\" style=\"background-color: rgba(231, 230, 241, 0.5);\">who</span> <span title=\"0.199\" style=\"background-color: rgba(227, 226, 239, 0.5);\">were</span> <span title=\"0.227\" style=\"background-color: rgba(221, 221, 236, 0.5);\">working</span> <span title=\"0.246\" style=\"background-color: rgba(218, 218, 235, 0.5);\">on</span> <span title=\"0.258\" style=\"background-color: rgba(215, 215, 233, 0.5);\">creating</span> <span title=\"0.268\" style=\"background-color: rgba(214, 214, 233, 0.5);\">intelligent</span> <span title=\"0.269\" style=\"background-color: rgba(214, 214, 233, 0.5);\">machines</span> <span title=\"0.277\" style=\"background-color: rgba(212, 212, 232, 0.5);\">(</span> <span title=\"0.283\" style=\"background-color: rgba(210, 210, 231, 0.5);\">they</span> <span title=\"0.280\" style=\"background-color: rgba(211, 211, 231, 0.5);\">call</span> <span title=\"0.277\" style=\"background-color: rgba(212, 212, 232, 0.5);\">them</span> <span title=\"0.257\" style=\"background-color: rgba(216, 216, 234, 0.5);\">&quot;</span> <span title=\"0.230\" style=\"background-color: rgba(221, 221, 236, 0.5);\">xxunk</span> <span title=\"0.208\" style=\"background-color: rgba(225, 224, 238, 0.5);\">&quot;</span> <span title=\"0.192\" style=\"background-color: rgba(227, 226, 239, 0.5);\">)</span> <span title=\"0.212\" style=\"background-color: rgba(224, 223, 238, 0.5);\">by</span> <span title=\"0.214\" style=\"background-color: rgba(224, 223, 238, 0.5);\">mimicking</span> <span title=\"0.217\" style=\"background-color: rgba(223, 223, 237, 0.5);\">our</span> <span title=\"0.228\" style=\"background-color: rgba(221, 221, 236, 0.5);\">own</span> <span title=\"0.237\" style=\"background-color: rgba(220, 220, 236, 0.5);\">cognitive</span> <span title=\"0.232\" style=\"background-color: rgba(221, 220, 236, 0.5);\">processes</span> <span title=\"0.216\" style=\"background-color: rgba(223, 223, 237, 0.5);\">:</span> <span title=\"0.195\" style=\"background-color: rgba(227, 226, 239, 0.5);\">they</span> <span title=\"0.191\" style=\"background-color: rgba(228, 227, 239, 0.5);\">did</span> <span title=\"0.198\" style=\"background-color: rgba(227, 226, 239, 0.5);\">n&#x27;t</span> <span title=\"0.202\" style=\"background-color: rgba(226, 225, 239, 0.5);\">have</span> <span title=\"0.191\" style=\"background-color: rgba(228, 227, 239, 0.5);\">access</span> <span title=\"0.188\" style=\"background-color: rgba(228, 227, 239, 0.5);\">yet</span> <span title=\"0.194\" style=\"background-color: rgba(227, 226, 239, 0.5);\">even</span> <span title=\"0.187\" style=\"background-color: rgba(229, 227, 240, 0.5);\">though</span> <span title=\"0.174\" style=\"background-color: rgba(231, 229, 241, 0.5);\">their</span> <span title=\"0.168\" style=\"background-color: rgba(232, 230, 241, 0.5);\">research</span> <span title=\"0.188\" style=\"background-color: rgba(228, 227, 239, 0.5);\">was</span> <span title=\"0.219\" style=\"background-color: rgba(223, 222, 237, 0.5);\">just</span> <span title=\"0.242\" style=\"background-color: rgba(219, 219, 235, 0.5);\">getting</span> <span title=\"0.244\" style=\"background-color: rgba(219, 219, 235, 0.5);\">started</span> <span title=\"0.219\" style=\"background-color: rgba(223, 222, 237, 0.5);\">…</span> <span title=\"0.189\" style=\"background-color: rgba(228, 227, 239, 0.5);\">xxmaj</span> <span title=\"0.175\" style=\"background-color: rgba(231, 229, 241, 0.5);\">they</span> <span title=\"0.167\" style=\"background-color: rgba(232, 230, 241, 0.5);\">showed</span> <span title=\"0.157\" style=\"background-color: rgba(233, 232, 242, 0.5);\">me</span> <span title=\"0.150\" style=\"background-color: rgba(234, 233, 243, 0.5);\">some</span> <span title=\"0.159\" style=\"background-color: rgba(233, 232, 242, 0.5);\">of</span> <span title=\"0.173\" style=\"background-color: rgba(231, 229, 241, 0.5);\">these</span> <span title=\"0.205\" style=\"background-color: rgba(225, 225, 238, 0.5);\">xxunk</span> <span title=\"0.254\" style=\"background-color: rgba(217, 217, 234, 0.5);\">systems</span> <span title=\"0.308\" style=\"background-color: rgba(204, 205, 228, 0.5);\">running</span> <span title=\"0.371\" style=\"background-color: rgba(189, 190, 220, 0.5);\">around</span> <span title=\"0.458\" style=\"background-color: rgba(167, 165, 206, 0.5);\">or</span> <span title=\"0.573\" style=\"background-color: rgba(140, 137, 191, 0.5);\">sitting</span> <span title=\"0.727\" style=\"background-color: rgba(109, 88, 166, 0.5);\">behind</span> <span title=\"0.888\" style=\"background-color: rgba(81, 34, 140, 0.5);\">glass</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">displays</span> <span title=\"0.963\" style=\"background-color: rgba(68, 11, 130, 0.5);\">somewhere</span> <span title=\"0.631\" style=\"background-color: rgba(126, 122, 184, 0.5);\">!</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "This caused quite significant concern among my atheist friends because we know from neuroscience how humans can get hooked into certain patterns when watching movies like Avatar, but what if such things happened inside computers instead?!\n",
            "cat=S confidence for R=0.4641110599040985 confidence for S=0.5358889102935791\n",
            "delta=0.07177785038948059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.493\" style=\"background-color: rgba(159, 155, 200, 0.5);\">xxbos</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">xxmaj</span> <span title=\"0.302\" style=\"background-color: rgba(205, 205, 228, 0.5);\">this</span> <span title=\"0.281\" style=\"background-color: rgba(210, 210, 231, 0.5);\">caused</span> <span title=\"0.244\" style=\"background-color: rgba(219, 219, 235, 0.5);\">quite</span> <span title=\"0.228\" style=\"background-color: rgba(221, 221, 236, 0.5);\">significant</span> <span title=\"0.235\" style=\"background-color: rgba(220, 220, 236, 0.5);\">concern</span> <span title=\"0.248\" style=\"background-color: rgba(218, 218, 235, 0.5);\">among</span> <span title=\"0.257\" style=\"background-color: rgba(216, 216, 234, 0.5);\">my</span> <span title=\"0.251\" style=\"background-color: rgba(217, 217, 234, 0.5);\">atheist</span> <span title=\"0.237\" style=\"background-color: rgba(220, 220, 236, 0.5);\">friends</span> <span title=\"0.213\" style=\"background-color: rgba(224, 223, 238, 0.5);\">because</span> <span title=\"0.204\" style=\"background-color: rgba(225, 225, 238, 0.5);\">we</span> <span title=\"0.215\" style=\"background-color: rgba(224, 223, 238, 0.5);\">know</span> <span title=\"0.231\" style=\"background-color: rgba(221, 220, 236, 0.5);\">from</span> <span title=\"0.250\" style=\"background-color: rgba(217, 217, 234, 0.5);\">neuroscience</span> <span title=\"0.267\" style=\"background-color: rgba(214, 214, 233, 0.5);\">how</span> <span title=\"0.271\" style=\"background-color: rgba(213, 213, 232, 0.5);\">humans</span> <span title=\"0.257\" style=\"background-color: rgba(216, 216, 234, 0.5);\">can</span> <span title=\"0.240\" style=\"background-color: rgba(219, 219, 235, 0.5);\">get</span> <span title=\"0.249\" style=\"background-color: rgba(218, 218, 235, 0.5);\">hooked</span> <span title=\"0.275\" style=\"background-color: rgba(212, 212, 232, 0.5);\">into</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">certain</span> <span title=\"0.305\" style=\"background-color: rgba(204, 205, 228, 0.5);\">patterns</span> <span title=\"0.295\" style=\"background-color: rgba(207, 207, 229, 0.5);\">when</span> <span title=\"0.291\" style=\"background-color: rgba(208, 208, 230, 0.5);\">watching</span> <span title=\"0.301\" style=\"background-color: rgba(205, 205, 228, 0.5);\">movies</span> <span title=\"0.327\" style=\"background-color: rgba(199, 200, 225, 0.5);\">like</span> <span title=\"0.331\" style=\"background-color: rgba(198, 199, 225, 0.5);\">xxmaj</span> <span title=\"0.316\" style=\"background-color: rgba(202, 203, 227, 0.5);\">avatar</span> <span title=\"0.327\" style=\"background-color: rgba(199, 200, 225, 0.5);\">,</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">but</span> <span title=\"0.332\" style=\"background-color: rgba(198, 198, 225, 0.5);\">what</span> <span title=\"0.339\" style=\"background-color: rgba(197, 197, 224, 0.5);\">if</span> <span title=\"0.376\" style=\"background-color: rgba(187, 188, 219, 0.5);\">such</span> <span title=\"0.462\" style=\"background-color: rgba(166, 164, 205, 0.5);\">things</span> <span title=\"0.585\" style=\"background-color: rgba(137, 134, 190, 0.5);\">happened</span> <span title=\"0.736\" style=\"background-color: rgba(108, 85, 165, 0.5);\">inside</span> <span title=\"0.883\" style=\"background-color: rgba(82, 36, 141, 0.5);\">computers</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">instead</span> <span title=\"0.965\" style=\"background-color: rgba(68, 11, 130, 0.5);\">?</span> <span title=\"0.636\" style=\"background-color: rgba(126, 121, 184, 0.5);\">!</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In fact it seems likely many people could become convinced about AI without ever having seen one personally.\n",
            "cat=S confidence for R=0.46411409974098206 confidence for S=0.5358859300613403\n",
            "delta=0.07177183032035828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.537\" style=\"background-color: rgba(149, 145, 195, 0.5);\">xxbos</span> <span title=\"0.393\" style=\"background-color: rgba(183, 184, 217, 0.5);\">xxmaj</span> <span title=\"0.380\" style=\"background-color: rgba(186, 187, 219, 0.5);\">in</span> <span title=\"0.379\" style=\"background-color: rgba(186, 187, 219, 0.5);\">fact</span> <span title=\"0.388\" style=\"background-color: rgba(184, 185, 217, 0.5);\">it</span> <span title=\"0.403\" style=\"background-color: rgba(181, 180, 215, 0.5);\">seems</span> <span title=\"0.411\" style=\"background-color: rgba(179, 178, 214, 0.5);\">likely</span> <span title=\"0.415\" style=\"background-color: rgba(178, 177, 213, 0.5);\">many</span> <span title=\"0.411\" style=\"background-color: rgba(179, 178, 214, 0.5);\">people</span> <span title=\"0.416\" style=\"background-color: rgba(178, 177, 213, 0.5);\">could</span> <span title=\"0.425\" style=\"background-color: rgba(176, 175, 212, 0.5);\">become</span> <span title=\"0.443\" style=\"background-color: rgba(171, 169, 209, 0.5);\">convinced</span> <span title=\"0.490\" style=\"background-color: rgba(160, 156, 201, 0.5);\">about</span> <span title=\"0.513\" style=\"background-color: rgba(154, 150, 198, 0.5);\">xxup</span> <span title=\"0.519\" style=\"background-color: rgba(153, 149, 198, 0.5);\">ai</span> <span title=\"0.565\" style=\"background-color: rgba(142, 138, 192, 0.5);\">without</span> <span title=\"0.645\" style=\"background-color: rgba(124, 117, 181, 0.5);\">ever</span> <span title=\"0.764\" style=\"background-color: rgba(103, 76, 160, 0.5);\">having</span> <span title=\"0.891\" style=\"background-color: rgba(80, 33, 140, 0.5);\">seen</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">one</span> <span title=\"0.949\" style=\"background-color: rgba(71, 15, 132, 0.5);\">personally</span> <span title=\"0.632\" style=\"background-color: rgba(126, 122, 184, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now imagine being confronted not only once before you die against your will through science fiction films; now think back another time after death where religious beliefs still play important roles….\n",
            "cat=S confidence for R=0.4641912877559662 confidence for S=0.5358086824417114\n",
            "delta=0.07161739468574524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.494\" style=\"background-color: rgba(159, 155, 200, 0.5);\">xxbos</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">xxmaj</span> <span title=\"0.283\" style=\"background-color: rgba(210, 210, 231, 0.5);\">now</span> <span title=\"0.251\" style=\"background-color: rgba(217, 217, 234, 0.5);\">imagine</span> <span title=\"0.224\" style=\"background-color: rgba(222, 222, 237, 0.5);\">being</span> <span title=\"0.222\" style=\"background-color: rgba(223, 222, 237, 0.5);\">confronted</span> <span title=\"0.238\" style=\"background-color: rgba(220, 220, 236, 0.5);\">not</span> <span title=\"0.247\" style=\"background-color: rgba(218, 218, 235, 0.5);\">only</span> <span title=\"0.243\" style=\"background-color: rgba(219, 219, 235, 0.5);\">once</span> <span title=\"0.233\" style=\"background-color: rgba(221, 220, 236, 0.5);\">before</span> <span title=\"0.244\" style=\"background-color: rgba(219, 219, 235, 0.5);\">you</span> <span title=\"0.261\" style=\"background-color: rgba(215, 215, 233, 0.5);\">die</span> <span title=\"0.270\" style=\"background-color: rgba(213, 213, 232, 0.5);\">against</span> <span title=\"0.287\" style=\"background-color: rgba(209, 209, 230, 0.5);\">your</span> <span title=\"0.321\" style=\"background-color: rgba(200, 201, 226, 0.5);\">will</span> <span title=\"0.333\" style=\"background-color: rgba(198, 198, 225, 0.5);\">through</span> <span title=\"0.320\" style=\"background-color: rgba(200, 201, 226, 0.5);\">science</span> <span title=\"0.301\" style=\"background-color: rgba(205, 205, 228, 0.5);\">fiction</span> <span title=\"0.269\" style=\"background-color: rgba(214, 214, 233, 0.5);\">films</span> <span title=\"0.265\" style=\"background-color: rgba(214, 215, 233, 0.5);\">;</span> <span title=\"0.279\" style=\"background-color: rgba(211, 211, 231, 0.5);\">now</span> <span title=\"0.298\" style=\"background-color: rgba(206, 206, 229, 0.5);\">think</span> <span title=\"0.321\" style=\"background-color: rgba(200, 201, 226, 0.5);\">back</span> <span title=\"0.335\" style=\"background-color: rgba(198, 198, 225, 0.5);\">another</span> <span title=\"0.347\" style=\"background-color: rgba(195, 195, 223, 0.5);\">time</span> <span title=\"0.344\" style=\"background-color: rgba(195, 195, 223, 0.5);\">after</span> <span title=\"0.341\" style=\"background-color: rgba(196, 196, 224, 0.5);\">death</span> <span title=\"0.359\" style=\"background-color: rgba(192, 193, 222, 0.5);\">where</span> <span title=\"0.404\" style=\"background-color: rgba(181, 180, 215, 0.5);\">religious</span> <span title=\"0.487\" style=\"background-color: rgba(161, 157, 202, 0.5);\">beliefs</span> <span title=\"0.603\" style=\"background-color: rgba(133, 129, 188, 0.5);\">still</span> <span title=\"0.732\" style=\"background-color: rgba(108, 86, 166, 0.5);\">play</span> <span title=\"0.881\" style=\"background-color: rgba(82, 36, 141, 0.5);\">important</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">roles</span> <span title=\"0.968\" style=\"background-color: rgba(68, 9, 129, 0.5);\">…</span> <span title=\"0.648\" style=\"background-color: rgba(124, 117, 181, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What do atheists need more than anything else right then!?\n",
            "cat=S confidence for R=0.464132696390152 confidence for S=0.5358673334121704\n",
            "delta=0.07173463702201843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.624\" style=\"background-color: rgba(128, 125, 186, 0.5);\">xxbos</span> <span title=\"0.496\" style=\"background-color: rgba(158, 154, 200, 0.5);\">xxmaj</span> <span title=\"0.478\" style=\"background-color: rgba(163, 160, 203, 0.5);\">what</span> <span title=\"0.462\" style=\"background-color: rgba(166, 164, 205, 0.5);\">do</span> <span title=\"0.493\" style=\"background-color: rgba(159, 155, 200, 0.5);\">atheists</span> <span title=\"0.548\" style=\"background-color: rgba(146, 142, 194, 0.5);\">need</span> <span title=\"0.613\" style=\"background-color: rgba(131, 128, 187, 0.5);\">more</span> <span title=\"0.666\" style=\"background-color: rgba(120, 110, 178, 0.5);\">than</span> <span title=\"0.728\" style=\"background-color: rgba(109, 88, 166, 0.5);\">anything</span> <span title=\"0.819\" style=\"background-color: rgba(93, 57, 151, 0.5);\">else</span> <span title=\"0.937\" style=\"background-color: rgba(73, 19, 134, 0.5);\">right</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">then</span> <span title=\"0.946\" style=\"background-color: rgba(71, 15, 132, 0.5);\">!</span> <span title=\"0.624\" style=\"background-color: rgba(128, 125, 186, 0.5);\">?</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "So here comes Dr Novella again talking nonsense...\n",
            "cat=S confidence for R=0.4640997052192688 confidence for S=0.5359002947807312\n",
            "delta=0.0718005895614624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.650\" style=\"background-color: rgba(123, 115, 181, 0.5);\">xxbos</span> <span title=\"0.534\" style=\"background-color: rgba(150, 146, 196, 0.5);\">xxmaj</span> <span title=\"0.518\" style=\"background-color: rgba(153, 149, 198, 0.5);\">so</span> <span title=\"0.510\" style=\"background-color: rgba(155, 151, 198, 0.5);\">here</span> <span title=\"0.530\" style=\"background-color: rgba(150, 147, 196, 0.5);\">comes</span> <span title=\"0.570\" style=\"background-color: rgba(141, 138, 192, 0.5);\">xxmaj</span> <span title=\"0.623\" style=\"background-color: rgba(128, 125, 186, 0.5);\">dr</span> <span title=\"0.714\" style=\"background-color: rgba(112, 93, 169, 0.5);\">xxmaj</span> <span title=\"0.816\" style=\"background-color: rgba(94, 58, 152, 0.5);\">novella</span> <span title=\"0.920\" style=\"background-color: rgba(76, 24, 136, 0.5);\">again</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">talking</span> <span title=\"0.948\" style=\"background-color: rgba(71, 15, 132, 0.5);\">nonsense</span> <span title=\"0.613\" style=\"background-color: rgba(130, 127, 187, 0.5);\">...</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "He thinks he has discovered evidence which shows 'god' exists - although obviously nothing suggests otherwise either way!!\n",
            "cat=S confidence for R=0.46415865421295166 confidence for S=0.5358414053916931\n",
            "delta=0.07168275117874146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.512\" style=\"background-color: rgba(154, 150, 198, 0.5);\">xxbos</span> <span title=\"0.355\" style=\"background-color: rgba(193, 194, 222, 0.5);\">xxmaj</span> <span title=\"0.332\" style=\"background-color: rgba(198, 198, 225, 0.5);\">he</span> <span title=\"0.323\" style=\"background-color: rgba(200, 201, 226, 0.5);\">thinks</span> <span title=\"0.314\" style=\"background-color: rgba(202, 203, 227, 0.5);\">he</span> <span title=\"0.302\" style=\"background-color: rgba(205, 205, 228, 0.5);\">has</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">discovered</span> <span title=\"0.307\" style=\"background-color: rgba(204, 205, 228, 0.5);\">evidence</span> <span title=\"0.321\" style=\"background-color: rgba(200, 201, 226, 0.5);\">which</span> <span title=\"0.355\" style=\"background-color: rgba(193, 194, 222, 0.5);\">shows</span> <span title=\"0.391\" style=\"background-color: rgba(183, 184, 217, 0.5);\">&#x27;</span> <span title=\"0.417\" style=\"background-color: rgba(178, 177, 213, 0.5);\">god</span> <span title=\"0.430\" style=\"background-color: rgba(174, 173, 210, 0.5);\">&#x27;</span> <span title=\"0.430\" style=\"background-color: rgba(174, 173, 210, 0.5);\">exists</span> <span title=\"0.402\" style=\"background-color: rgba(182, 182, 216, 0.5);\">-</span> <span title=\"0.408\" style=\"background-color: rgba(180, 179, 214, 0.5);\">although</span> <span title=\"0.446\" style=\"background-color: rgba(170, 168, 208, 0.5);\">obviously</span> <span title=\"0.528\" style=\"background-color: rgba(150, 147, 196, 0.5);\">nothing</span> <span title=\"0.643\" style=\"background-color: rgba(124, 118, 182, 0.5);\">suggests</span> <span title=\"0.774\" style=\"background-color: rgba(101, 72, 158, 0.5);\">otherwise</span> <span title=\"0.895\" style=\"background-color: rgba(80, 31, 139, 0.5);\">either</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">way</span> <span title=\"0.974\" style=\"background-color: rgba(66, 7, 128, 0.5);\">!</span> <span title=\"0.645\" style=\"background-color: rgba(124, 117, 181, 0.5);\">!</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "For example his idea would imply human beings create god out-of thin air so why don't aliens also exist???\n",
            "cat=S confidence for R=0.46422529220581055 confidence for S=0.5357747077941895\n",
            "delta=0.0715494155883789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.541\" style=\"background-color: rgba(148, 144, 195, 0.5);\">xxbos</span> <span title=\"0.373\" style=\"background-color: rgba(188, 189, 220, 0.5);\">xxmaj</span> <span title=\"0.347\" style=\"background-color: rgba(195, 195, 223, 0.5);\">for</span> <span title=\"0.330\" style=\"background-color: rgba(198, 199, 225, 0.5);\">example</span> <span title=\"0.316\" style=\"background-color: rgba(202, 203, 227, 0.5);\">his</span> <span title=\"0.315\" style=\"background-color: rgba(202, 203, 227, 0.5);\">idea</span> <span title=\"0.320\" style=\"background-color: rgba(201, 202, 226, 0.5);\">would</span> <span title=\"0.342\" style=\"background-color: rgba(196, 196, 224, 0.5);\">imply</span> <span title=\"0.363\" style=\"background-color: rgba(191, 192, 221, 0.5);\">human</span> <span title=\"0.378\" style=\"background-color: rgba(187, 188, 219, 0.5);\">beings</span> <span title=\"0.386\" style=\"background-color: rgba(185, 186, 218, 0.5);\">create</span> <span title=\"0.370\" style=\"background-color: rgba(189, 190, 220, 0.5);\">god</span> <span title=\"0.349\" style=\"background-color: rgba(194, 195, 223, 0.5);\">out</span> <span title=\"0.346\" style=\"background-color: rgba(195, 195, 223, 0.5);\">-</span> <span title=\"0.332\" style=\"background-color: rgba(198, 199, 225, 0.5);\">of</span> <span title=\"0.325\" style=\"background-color: rgba(199, 200, 225, 0.5);\">thin</span> <span title=\"0.319\" style=\"background-color: rgba(201, 202, 226, 0.5);\">air</span> <span title=\"0.331\" style=\"background-color: rgba(198, 199, 225, 0.5);\">so</span> <span title=\"0.350\" style=\"background-color: rgba(194, 195, 223, 0.5);\">why</span> <span title=\"0.392\" style=\"background-color: rgba(183, 184, 217, 0.5);\">do</span> <span title=\"0.462\" style=\"background-color: rgba(166, 164, 205, 0.5);\">n&#x27;t</span> <span title=\"0.591\" style=\"background-color: rgba(135, 132, 189, 0.5);\">aliens</span> <span title=\"0.747\" style=\"background-color: rgba(106, 81, 163, 0.5);\">also</span> <span title=\"0.902\" style=\"background-color: rgba(78, 29, 138, 0.5);\">exist</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">?</span> <span title=\"0.964\" style=\"background-color: rgba(68, 11, 130, 0.5);\">?</span> <span title=\"0.639\" style=\"background-color: rgba(125, 119, 183, 0.5);\">?</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actually Dawkins himself says much better examples suggest creationism rather Gods existence!\n",
            "cat=S confidence for R=0.4641396701335907 confidence for S=0.5358603000640869\n",
            "delta=0.07172062993049622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.576\" style=\"background-color: rgba(139, 136, 191, 0.5);\">xxbos</span> <span title=\"0.478\" style=\"background-color: rgba(163, 160, 203, 0.5);\">xxmaj</span> <span title=\"0.477\" style=\"background-color: rgba(163, 160, 203, 0.5);\">actually</span> <span title=\"0.468\" style=\"background-color: rgba(166, 163, 205, 0.5);\">xxmaj</span> <span title=\"0.446\" style=\"background-color: rgba(170, 168, 208, 0.5);\">dawkins</span> <span title=\"0.451\" style=\"background-color: rgba(169, 167, 207, 0.5);\">himself</span> <span title=\"0.475\" style=\"background-color: rgba(164, 161, 204, 0.5);\">says</span> <span title=\"0.508\" style=\"background-color: rgba(155, 151, 198, 0.5);\">much</span> <span title=\"0.552\" style=\"background-color: rgba(145, 141, 194, 0.5);\">better</span> <span title=\"0.600\" style=\"background-color: rgba(134, 130, 188, 0.5);\">examples</span> <span title=\"0.651\" style=\"background-color: rgba(123, 115, 181, 0.5);\">suggest</span> <span title=\"0.690\" style=\"background-color: rgba(116, 102, 174, 0.5);\">creationism</span> <span title=\"0.766\" style=\"background-color: rgba(102, 74, 160, 0.5);\">rather</span> <span title=\"0.904\" style=\"background-color: rgba(78, 29, 138, 0.5);\">xxmaj</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">gods</span> <span title=\"0.956\" style=\"background-color: rgba(70, 13, 131, 0.5);\">existence</span> <span title=\"0.622\" style=\"background-color: rgba(128, 125, 186, 0.5);\">!</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Also remember Einstein said whatever created us did NOT use magic technology....\n",
            "cat=S confidence for R=0.46411845088005066 confidence for S=0.5358815789222717\n",
            "delta=0.07176312804222107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.534\" style=\"background-color: rgba(150, 146, 196, 0.5);\">xxbos</span> <span title=\"0.396\" style=\"background-color: rgba(182, 183, 216, 0.5);\">xxmaj</span> <span title=\"0.388\" style=\"background-color: rgba(184, 185, 217, 0.5);\">also</span> <span title=\"0.363\" style=\"background-color: rgba(190, 191, 221, 0.5);\">remember</span> <span title=\"0.342\" style=\"background-color: rgba(196, 196, 224, 0.5);\">xxmaj</span> <span title=\"0.335\" style=\"background-color: rgba(198, 198, 225, 0.5);\">einstein</span> <span title=\"0.364\" style=\"background-color: rgba(190, 191, 221, 0.5);\">said</span> <span title=\"0.411\" style=\"background-color: rgba(179, 178, 214, 0.5);\">whatever</span> <span title=\"0.460\" style=\"background-color: rgba(167, 165, 206, 0.5);\">created</span> <span title=\"0.493\" style=\"background-color: rgba(159, 155, 200, 0.5);\">us</span> <span title=\"0.498\" style=\"background-color: rgba(158, 154, 200, 0.5);\">did</span> <span title=\"0.513\" style=\"background-color: rgba(154, 150, 198, 0.5);\">xxup</span> <span title=\"0.584\" style=\"background-color: rgba(137, 134, 190, 0.5);\">not</span> <span title=\"0.678\" style=\"background-color: rgba(118, 106, 176, 0.5);\">use</span> <span title=\"0.793\" style=\"background-color: rgba(98, 66, 156, 0.5);\">magic</span> <span title=\"0.912\" style=\"background-color: rgba(77, 26, 137, 0.5);\">technology</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">xxrep</span> <span title=\"0.939\" style=\"background-color: rgba(72, 18, 133, 0.5);\">4</span> <span title=\"0.604\" style=\"background-color: rgba(133, 129, 188, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "But since most religions say everything must come directly form/be produced via supernatural power Why does Christianity believe Jesus rose bodily up onto earth AFTER dying?????\n",
            "cat=S confidence for R=0.46416258811950684 confidence for S=0.5358374118804932\n",
            "delta=0.07167482376098633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.478\" style=\"background-color: rgba(163, 160, 203, 0.5);\">xxbos</span> <span title=\"0.294\" style=\"background-color: rgba(207, 207, 229, 0.5);\">xxmaj</span> <span title=\"0.266\" style=\"background-color: rgba(214, 214, 233, 0.5);\">but</span> <span title=\"0.252\" style=\"background-color: rgba(217, 217, 234, 0.5);\">since</span> <span title=\"0.242\" style=\"background-color: rgba(219, 219, 235, 0.5);\">most</span> <span title=\"0.233\" style=\"background-color: rgba(221, 220, 236, 0.5);\">religions</span> <span title=\"0.233\" style=\"background-color: rgba(221, 220, 236, 0.5);\">say</span> <span title=\"0.232\" style=\"background-color: rgba(221, 220, 236, 0.5);\">everything</span> <span title=\"0.237\" style=\"background-color: rgba(220, 220, 236, 0.5);\">must</span> <span title=\"0.239\" style=\"background-color: rgba(219, 219, 235, 0.5);\">come</span> <span title=\"0.261\" style=\"background-color: rgba(215, 215, 233, 0.5);\">directly</span> <span title=\"0.289\" style=\"background-color: rgba(209, 209, 230, 0.5);\">form</span> <span title=\"0.304\" style=\"background-color: rgba(205, 205, 228, 0.5);\">/</span> <span title=\"0.300\" style=\"background-color: rgba(206, 206, 229, 0.5);\">be</span> <span title=\"0.308\" style=\"background-color: rgba(204, 205, 228, 0.5);\">produced</span> <span title=\"0.321\" style=\"background-color: rgba(200, 201, 226, 0.5);\">via</span> <span title=\"0.312\" style=\"background-color: rgba(203, 204, 227, 0.5);\">supernatural</span> <span title=\"0.282\" style=\"background-color: rgba(210, 210, 231, 0.5);\">power</span> <span title=\"0.262\" style=\"background-color: rgba(214, 215, 233, 0.5);\">xxmaj</span> <span title=\"0.267\" style=\"background-color: rgba(214, 214, 233, 0.5);\">why</span> <span title=\"0.269\" style=\"background-color: rgba(214, 214, 233, 0.5);\">does</span> <span title=\"0.270\" style=\"background-color: rgba(213, 213, 232, 0.5);\">xxmaj</span> <span title=\"0.260\" style=\"background-color: rgba(215, 215, 233, 0.5);\">christianity</span> <span title=\"0.255\" style=\"background-color: rgba(216, 216, 234, 0.5);\">believe</span> <span title=\"0.250\" style=\"background-color: rgba(217, 217, 234, 0.5);\">xxmaj</span> <span title=\"0.257\" style=\"background-color: rgba(216, 216, 234, 0.5);\">jesus</span> <span title=\"0.291\" style=\"background-color: rgba(208, 208, 230, 0.5);\">rose</span> <span title=\"0.329\" style=\"background-color: rgba(198, 199, 225, 0.5);\">bodily</span> <span title=\"0.368\" style=\"background-color: rgba(189, 190, 220, 0.5);\">up</span> <span title=\"0.410\" style=\"background-color: rgba(180, 179, 214, 0.5);\">onto</span> <span title=\"0.479\" style=\"background-color: rgba(163, 160, 203, 0.5);\">earth</span> <span title=\"0.580\" style=\"background-color: rgba(138, 135, 190, 0.5);\">xxup</span> <span title=\"0.741\" style=\"background-color: rgba(107, 84, 164, 0.5);\">after</span> <span title=\"0.897\" style=\"background-color: rgba(80, 31, 139, 0.5);\">dying</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">xxrep</span> <span title=\"0.947\" style=\"background-color: rgba(71, 15, 132, 0.5);\">5</span> <span title=\"0.619\" style=\"background-color: rgba(129, 126, 186, 0.5);\">?</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "And look.. its really hard work trying keep track down contradictions between different versions thereof itself :) Just ignore those idiots claiming Christians invented evolution!!!\n",
            "cat=S confidence for R=0.46417656540870667 confidence for S=0.5358234643936157\n",
            "delta=0.07164689898490906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.505\" style=\"background-color: rgba(156, 152, 199, 0.5);\">xxbos</span> <span title=\"0.310\" style=\"background-color: rgba(203, 204, 227, 0.5);\">xxmaj</span> <span title=\"0.299\" style=\"background-color: rgba(206, 206, 229, 0.5);\">and</span> <span title=\"0.271\" style=\"background-color: rgba(213, 213, 232, 0.5);\">look</span> <span title=\"0.240\" style=\"background-color: rgba(219, 219, 235, 0.5);\">..</span> <span title=\"0.229\" style=\"background-color: rgba(221, 221, 236, 0.5);\">its</span> <span title=\"0.234\" style=\"background-color: rgba(221, 220, 236, 0.5);\">really</span> <span title=\"0.257\" style=\"background-color: rgba(216, 216, 234, 0.5);\">hard</span> <span title=\"0.285\" style=\"background-color: rgba(209, 209, 230, 0.5);\">work</span> <span title=\"0.303\" style=\"background-color: rgba(205, 205, 228, 0.5);\">trying</span> <span title=\"0.307\" style=\"background-color: rgba(204, 205, 228, 0.5);\">keep</span> <span title=\"0.295\" style=\"background-color: rgba(207, 207, 229, 0.5);\">track</span> <span title=\"0.295\" style=\"background-color: rgba(207, 207, 229, 0.5);\">down</span> <span title=\"0.303\" style=\"background-color: rgba(205, 205, 228, 0.5);\">contradictions</span> <span title=\"0.302\" style=\"background-color: rgba(205, 205, 228, 0.5);\">between</span> <span title=\"0.306\" style=\"background-color: rgba(204, 205, 228, 0.5);\">different</span> <span title=\"0.307\" style=\"background-color: rgba(204, 205, 228, 0.5);\">versions</span> <span title=\"0.306\" style=\"background-color: rgba(204, 205, 228, 0.5);\">thereof</span> <span title=\"0.292\" style=\"background-color: rgba(208, 208, 230, 0.5);\">itself</span> <span title=\"0.278\" style=\"background-color: rgba(211, 211, 231, 0.5);\">xxunk</span> <span title=\"0.271\" style=\"background-color: rgba(213, 213, 232, 0.5);\">xxmaj</span> <span title=\"0.289\" style=\"background-color: rgba(208, 208, 230, 0.5);\">just</span> <span title=\"0.306\" style=\"background-color: rgba(204, 205, 228, 0.5);\">ignore</span> <span title=\"0.326\" style=\"background-color: rgba(199, 200, 225, 0.5);\">those</span> <span title=\"0.358\" style=\"background-color: rgba(192, 193, 222, 0.5);\">idiots</span> <span title=\"0.425\" style=\"background-color: rgba(176, 175, 212, 0.5);\">claiming</span> <span title=\"0.531\" style=\"background-color: rgba(150, 147, 196, 0.5);\">xxmaj</span> <span title=\"0.658\" style=\"background-color: rgba(122, 113, 179, 0.5);\">christians</span> <span title=\"0.791\" style=\"background-color: rgba(98, 66, 156, 0.5);\">invented</span> <span title=\"0.922\" style=\"background-color: rgba(75, 23, 135, 0.5);\">evolution</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">!</span> <span title=\"0.956\" style=\"background-color: rgba(70, 13, 131, 0.5);\">!</span> <span title=\"0.631\" style=\"background-color: rgba(126, 122, 184, 0.5);\">!</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________\n",
            "cat=S confidence for R=0.46389463543891907 confidence for S=0.5361053943634033\n",
            "delta=0.07221075892448425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">xxbos</span> <span title=\"0.941\" style=\"background-color: rgba(72, 18, 133, 0.5);\">xxrep</span> <span title=\"0.854\" style=\"background-color: rgba(87, 45, 146, 0.5);\">18</span> <span title=\"0.555\" style=\"background-color: rgba(144, 140, 193, 0.5);\">_</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
            "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-b3bd5f9acb96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_intrinsic_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPurples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'press enter to continue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BITLZcM0qSa2",
        "colab": {}
      },
      "source": [
        "df_predict.dtypes\n",
        "df_predict=df_predict.astype({'Chapter':'int64'},{'Verse':'int64'})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JfHYiEnQlOSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "5eddaeb8-ebaa-4c50-87d3-2c5bd05797be"
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "print(df_predict.loc[(df_predict['Verse']==verse) & (df_predict['Chapter']==chapter)])\n",
        "\n",
        "interp.show_intrinsic_attention(check_text,cmap=cm.Purples)\n",
        "interp.intrinsic_attention(check_text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Book  Chapter Verse   Cat  Non-paul_likelihood  Paul_likelihood\n",
            "2  Ephesians        1     3  Paul             0.036072         0.963928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.113\" style=\"background-color: rgba(240, 238, 245, 0.5);\">xxbos</span> <span title=\"0.153\" style=\"background-color: rgba(234, 232, 242, 0.5);\">xxmaj</span> <span title=\"0.905\" style=\"background-color: rgba(78, 29, 138, 0.5);\">blessed</span> <span title=\"0.617\" style=\"background-color: rgba(129, 126, 186, 0.5);\">be</span> <span title=\"0.369\" style=\"background-color: rgba(189, 190, 220, 0.5);\">the</span> <span title=\"0.336\" style=\"background-color: rgba(197, 197, 224, 0.5);\">xxmaj</span> <span title=\"0.635\" style=\"background-color: rgba(126, 121, 184, 0.5);\">god</span> <span title=\"0.541\" style=\"background-color: rgba(148, 144, 195, 0.5);\">and</span> <span title=\"0.293\" style=\"background-color: rgba(207, 207, 229, 0.5);\">xxmaj</span> <span title=\"0.736\" style=\"background-color: rgba(108, 85, 165, 0.5);\">father</span> <span title=\"0.418\" style=\"background-color: rgba(177, 176, 212, 0.5);\">of</span> <span title=\"0.634\" style=\"background-color: rgba(126, 121, 184, 0.5);\">our</span> <span title=\"0.337\" style=\"background-color: rgba(197, 197, 224, 0.5);\">xxmaj</span> <span title=\"0.629\" style=\"background-color: rgba(126, 122, 184, 0.5);\">lord</span> <span title=\"0.363\" style=\"background-color: rgba(191, 192, 221, 0.5);\">xxmaj</span> <span title=\"0.776\" style=\"background-color: rgba(101, 72, 158, 0.5);\">jesus</span> <span title=\"0.376\" style=\"background-color: rgba(187, 188, 219, 0.5);\">xxmaj</span> <span title=\"0.653\" style=\"background-color: rgba(122, 114, 180, 0.5);\">christ</span> <span title=\"0.161\" style=\"background-color: rgba(232, 231, 242, 0.5);\">,</span> <span title=\"0.238\" style=\"background-color: rgba(220, 220, 236, 0.5);\">who</span> <span title=\"0.379\" style=\"background-color: rgba(186, 187, 219, 0.5);\">has</span> <span title=\"0.773\" style=\"background-color: rgba(102, 73, 159, 0.5);\">blessed</span> <span title=\"0.641\" style=\"background-color: rgba(124, 118, 182, 0.5);\">us</span> <span title=\"0.373\" style=\"background-color: rgba(188, 189, 220, 0.5);\">with</span> <span title=\"0.370\" style=\"background-color: rgba(189, 190, 220, 0.5);\">every</span> <span title=\"0.558\" style=\"background-color: rgba(144, 140, 193, 0.5);\">spiritual</span> <span title=\"0.707\" style=\"background-color: rgba(113, 96, 171, 0.5);\">blessing</span> <span title=\"0.340\" style=\"background-color: rgba(196, 196, 224, 0.5);\">in</span> <span title=\"0.256\" style=\"background-color: rgba(216, 216, 234, 0.5);\">the</span> <span title=\"0.525\" style=\"background-color: rgba(151, 148, 197, 0.5);\">heavenly</span> <span title=\"0.680\" style=\"background-color: rgba(117, 104, 175, 0.5);\">places</span> <span title=\"0.534\" style=\"background-color: rgba(150, 146, 196, 0.5);\">in</span> <span title=\"0.331\" style=\"background-color: rgba(198, 199, 225, 0.5);\">xxmaj</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">christ</span> <span title=\"0.815\" style=\"background-color: rgba(94, 58, 152, 0.5);\">,</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos xxmaj blessed be the xxmaj god and xxmaj father of our xxmaj lord xxmaj jesus xxmaj christ , who has blessed us with every spiritual blessing in the heavenly places in xxmaj christ ,,\n",
              " tensor([0.1128, 0.1532, 0.9053, 0.6172, 0.3694, 0.3360, 0.6350, 0.5408, 0.2933,\n",
              "         0.7356, 0.4185, 0.6342, 0.3372, 0.6291, 0.3630, 0.7760, 0.3762, 0.6530,\n",
              "         0.1608, 0.2381, 0.3793, 0.7730, 0.6412, 0.3731, 0.3695, 0.5580, 0.7069,\n",
              "         0.3400, 0.2558, 0.5253, 0.6802, 0.5342, 0.3309, 1.0000, 0.8153],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qpgiQksen1ah",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}